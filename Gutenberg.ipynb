{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gutenberg",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3710jvsc74a57bd0d92a51622b296ba858cddf5b4b5fbdb14ae90b49e1bc32df09b628c060f66614",
      "display_name": "Python 3.7.10 64-bit ('tf': conda)"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.10"
    },
    "accelerator": "GPU",
    "metadata": {
      "interpreter": {
        "hash": "d92a51622b296ba858cddf5b4b5fbdb14ae90b49e1bc32df09b628c060f66614"
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAwPHEFBj_qN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce33c211-1c9a-46a0-af53-c1f06c047ced"
      },
      "source": [
        "where = \"home\" # \"home\" # \"colab\"\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import os\n",
        "import math\n",
        "\n",
        "if where == \"colab\":\n",
        "    import pkg_resources\n",
        "    pkg_resources.require(\"numpy==1.19.5\")  # modified to use specific numpy\n",
        "    import numpy as np\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "elif where == \"home\":\n",
        "    os.chdir(\"/home/camilo/anaconda3/envs/tf/lib/python3.7/site-packages\")\n",
        "    import pkg_resources\n",
        "    pkg_resources.require(\"numpy==1.19.5\")  # modified to use specific numpy\n",
        "    import numpy as np\n",
        "\n",
        "\n",
        "tf.config.list_physical_devices('GPU')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhWa48orkgqm"
      },
      "source": [
        "# CONFIGURATION ------ \n",
        "batch_size = 64  # Batch size for training.\n",
        "epochs = 100  # Number of epochs to train for.\n",
        "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
        "num_samples = 64  # Number of samples to train on.\n",
        "total= (num_samples+1)* 3000 #500000\n",
        "# Path to the data txt file on disk.\n",
        "save_per_n_epochs= 20\n",
        "\n",
        "# Dirs \n",
        "if where == \"colab\":\n",
        "    data_path = \"/content/drive/MyDrive/Gutenberg/test_2_half_M.txt\"\n",
        "    checkpoint_filepath= \"/content/drive/MyDrive/Gutenberg\"\n",
        "elif where ==\"home\":\n",
        "    data_path = \"/home/camilo/Documents/Own Projects/github/spelll4french/test_2_half_M.txt\"\n",
        "    checkpoint_filepath= \"/home/camilo/Documents/Own Projects/Gutenberg\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0g0oVGyFum6Z"
      },
      "source": [
        "# INPUT,TARGET LISTS AND PHRASES ------\n",
        "def init_lists():\n",
        "  input_texts = []\n",
        "  target_texts = []\n",
        "  return(input_texts, target_texts)\n",
        " \n",
        "def append_lines(line,input_list, target_list):\n",
        "  input_text, target_text = line.split(\"\\t\")\n",
        "  target_text = \"\\t\" + target_text + \"\\n\"\n",
        "  input_list.append(input_text)\n",
        "  target_list.append(target_text)\n",
        "  return(input_text, target_text)\n",
        " \n",
        "def extract_from_lines(line): \n",
        "  input_text, target_text = line.split(\"\\t\")\n",
        "  target_text = \"\\t\" + target_text + \"\\n\"\n",
        "  return(input_text, target_text)\n",
        " \n",
        "# CHARACTER DICTIONNARIES ------\n",
        " \n",
        "def init_dict_training():\n",
        "  input_characters = set()\n",
        "  target_characters = set()\n",
        "  return(input_characters,target_characters)\n",
        "  \n",
        "def look_if_in_dict(input_text, target_text,input_dict,target_dict):\n",
        "  for char in input_text:\n",
        "    if char not in input_characters:\n",
        "      input_dict.add(char)\n",
        "  for char in target_text:\n",
        "    if char not in target_characters:\n",
        "      target_dict.add(char)\n",
        " \n",
        "def put_in_dict(input_text, target_text,input_dict,target_dict):\n",
        "  for char in input_text:\n",
        "    if char not in input_characters: # probably remove the ifs since we already read all ? \n",
        "      input_dict.add(char)\n",
        "  for char in target_text:\n",
        "    if char not in target_characters:\n",
        "      target_dict.add(char)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSaJ54Ku1uSl"
      },
      "source": [
        "def get_batch_info():\n",
        " \n",
        "  max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "  max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        " \n",
        "  print(\"Number of samples:\", len(input_texts))\n",
        "  print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
        "  print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n",
        "  return(max_encoder_seq_length,max_decoder_seq_length)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdOERDrFyFs2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb08b9f0-2e83-4652-d90f-a36491edbd67"
      },
      "source": [
        "# GET ALL UNIQUE CHARACTERS IN A DICT ------\n",
        "input_characters,target_characters= init_dict_training() # create empty dict to hold characters\n",
        "max_encoder_seq_length= []\n",
        "max_decoder_seq_length= []\n",
        "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.read().split(\"\\n\")\n",
        "    print('Reading',len(lines), 'lines of the txt file' )\n",
        "for line in lines[0:len(lines)-1]:\n",
        "  input_text, target_text= extract_from_lines(line)\n",
        "  look_if_in_dict(input_text,target_text,input_characters,target_characters)\n",
        "  max_encoder_seq_length.append(len(input_text))\n",
        "  max_decoder_seq_length.append(len(target_text))\n",
        "# Get information about this batch\n",
        "#max_encoder_seq_length,max_decoder_seq_length= get_batch_info()\n",
        " \n",
        "input_characters = sorted(list(input_characters))\n",
        "target_characters = sorted(list(target_characters))\n",
        "num_encoder_tokens = len(input_characters)\n",
        "num_decoder_tokens = len(target_characters)\n",
        " \n",
        "print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
        "print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
        " \n",
        "# Dict and index for Input and Output\n",
        "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
        "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
        "\n",
        "# Reverse-lookup token index to decode sequences back to something readable.\n",
        "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
        " \n",
        "# Max sentence length \n",
        "max_encoder_seq_length= max(max_encoder_seq_length)\n",
        "max_decoder_seq_length= max(max_decoder_seq_length)\n",
        "print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
        "print(\"Max sequence length for outputs:\", max_decoder_seq_length)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading 502363 lines of the txt file\n",
            "Number of unique input tokens: 96\n",
            "Number of unique output tokens: 97\n",
            "Max sequence length for inputs: 79\n",
            "Max sequence length for outputs: 73\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlSfroP0fcKB"
      },
      "source": [
        "# MODEL \n",
        "# Define an input sequence and process it.\n",
        "encoder_inputs = keras.Input(shape=(None, num_encoder_tokens), name = \"Encoder input\")\n",
        "encoder = keras.layers.LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "# We discard `encoder_outputs` and only keep the states.\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = keras.Input(shape=(None, num_decoder_tokens), name= \"Decoder input\")\n",
        "\n",
        "# We set up our decoder to return full output sequences,\n",
        "# and to return internal states as well. We don't use the\n",
        "# return states in the training model, but we will use them in inference.\n",
        "decoder_lstm = keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\")\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model that will turn\n",
        "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "#model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model = keras.Model(inputs= [encoder_inputs, decoder_inputs],outputs= decoder_outputs)\n",
        "\n",
        "# callback \n",
        "#\"{epoch:02d}-{val_accuracy:.2f}.hdf5\",\n",
        "os.chdir(checkpoint_filepath)\n",
        "model_checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=\"{epoch:02d}.hdf5\",\n",
        "    save_weights_only=False,\n",
        "    monitor='val_accuracy',\n",
        "    save_freq= 7849*1, #'epoch', #int(0.8*num_samples/batch_size*save_per_n_epochs), # 80% training, 20% val\n",
        "    save_best_only=False)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nCim5eRf5bZ",
        "outputId": "5832d27d-984a-43e2-9591-03867b9294e4"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\nEncoder input (InputLayer)      [(None, None, 96)]   0                                            \n__________________________________________________________________________________________________\nDecoder input (InputLayer)      [(None, None, 97)]   0                                            \n__________________________________________________________________________________________________\nlstm (LSTM)                     [(None, 256), (None, 361472      Encoder input[0][0]              \n__________________________________________________________________________________________________\nlstm_1 (LSTM)                   [(None, None, 256),  362496      Decoder input[0][0]              \n                                                                 lstm[0][1]                       \n                                                                 lstm[0][2]                       \n__________________________________________________________________________________________________\ndense (Dense)                   (None, None, 97)     24929       lstm_1[0][0]                     \n==================================================================================================\nTotal params: 748,897\nTrainable params: 748,897\nNon-trainable params: 0\n__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEYr8Act0RL0",
        "cellView": "form"
      },
      "source": [
        "#@title CLASS npyGen\n",
        "class npyGen(tf.keras.utils.Sequence):   \n",
        "    def __init__(self, con,\n",
        "                 batch_size,#,shuffle=True\n",
        "                 is_val=False, val_split= 0.9):\n",
        "        self.connection= con \n",
        "\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Validation settings \n",
        "        self.is_val= is_val\n",
        "        self.val_split= val_split\n",
        "        self.cutoff= math.floor(self.batch_size*self.val_split)\n",
        "        \n",
        "        # Shuffle\n",
        "        #self.shuffle = shuffle # To Do \n",
        "        self.n = len(self.connection)\n",
        "\n",
        "    #def on_epoch_end(self):\n",
        "     #   if self.shuffle:\n",
        "     #      self.df.sample(frac=1).reset_index(drop=True)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        input_texts, target_texts= init_lists() # init lists\n",
        "\n",
        "        for line in lines[index* self.batch_size : (index+1)* self.batch_size ] :\n",
        "            input_text, target_text= append_lines(line,input_texts, target_texts)\n",
        "\n",
        "            # Remove \"\" from strings\n",
        "            input_texts = [rm.replace('\\\"', '') for rm in input_texts]\n",
        "            target_texts = [rm.replace('\\\"', '') for rm in target_texts]\n",
        "\n",
        "            encoder_input_data = np.zeros( \n",
        "                (len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\n",
        "            )\n",
        "            decoder_input_data = np.zeros(\n",
        "                (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
        "            )\n",
        "            decoder_target_data = np.zeros(\n",
        "                (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
        "            )\n",
        "\n",
        "\n",
        "        for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "            for t, char in enumerate(input_text):\n",
        "                encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
        "            encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0\n",
        "            for t, char in enumerate(target_text):\n",
        "                # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "                decoder_input_data[i, t, target_token_index[char]] = 1.0\n",
        "                if t > 0:\n",
        "                    # decoder_target_data will be ahead by one timestep\n",
        "                    # and will not include the start character.\n",
        "                    decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "            # Space for everything after\n",
        "            decoder_input_data[i, t + 1 :, target_token_index[\" \"]] = 1.0 \n",
        "            decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0\n",
        "\n",
        "        # Validation split \n",
        "        # Either way the output arrays will not be complete\n",
        "\n",
        "        if not self.is_val: # not val. Then training split\n",
        "          encoder_input= encoder_input_data[:self.cutoff]\n",
        "          decoder_input=decoder_input_data[:self.cutoff]\n",
        "          decoder_target=decoder_target_data[:self.cutoff]\n",
        "          \n",
        "\n",
        "        if self.is_val: # if val then val split\n",
        "          encoder_input= encoder_input_data[self.cutoff:]\n",
        "          decoder_input=decoder_input_data[self.cutoff:]\n",
        "          decoder_target=decoder_target_data[self.cutoff:]\n",
        "          \n",
        "        return [encoder_input, decoder_input], decoder_target\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.n // self.batch_size"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRuWPomBJUVF"
      },
      "source": [
        "if where == \"colab\":\n",
        "    model= keras.models.load_model('/content/drive/MyDrive/Gutenberg/10.hdf5')\n",
        "elif where == \"home\":\n",
        "    model= keras.models.load_model('/home/camilo/Documents/Own Projects/Gutenberg/Checkpoints/56.hdf5')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "MCXuLcCI8QeO",
        "outputId": "f26f2d97-b04b-4afc-9e22-4ad90277ba9f"
      },
      "source": [
        "# Model FITTING ---\n",
        "history= model.fit(\n",
        "npyGen(con= lines,batch_size=64, is_val= False),\n",
        "#batch_size=batch_size,\n",
        "epochs=1, #epochs-66,\n",
        "validation_data=npyGen(con= lines,batch_size=64, is_val=True),\n",
        "callbacks=[model_checkpoint_callback],\n",
        "initial_epoch=57\n",
        ")\n",
        "\n",
        "#Saving \n",
        "name_to_save= \"/content/drive/MyDrive/Gutenberg/\"+'model_'\n",
        "model.save(name_to_save)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "   2/7849 [..............................] - ETA: 1:34:52 - loss: 0.4833 - accuracy: 0.8504"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-2ae5ba145438>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnpyGen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcon\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_checkpoint_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    922\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3024\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1961\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\nEncoder input (InputLayer)      [(None, None, 96)]   0                                            \n__________________________________________________________________________________________________\nDecoder input (InputLayer)      [(None, None, 97)]   0                                            \n__________________________________________________________________________________________________\nlstm (LSTM)                     [(None, 256), (None, 361472      Encoder input[0][0]              \n__________________________________________________________________________________________________\nlstm_1 (LSTM)                   [(None, None, 256),  362496      Decoder input[0][0]              \n                                                                 lstm[0][1]                       \n                                                                 lstm[0][2]                       \n__________________________________________________________________________________________________\ndense (Dense)                   (None, None, 97)     24929       lstm_1[0][0]                     \n==================================================================================================\nTotal params: 748,897\nTrainable params: 748,897\nNon-trainable params: 0\n__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<KerasTensor: shape=(None, 256) dtype=float32 (created by layer 'lstm')>,\n",
              " <KerasTensor: shape=(None, 256) dtype=float32 (created by layer 'lstm')>,\n",
              " <KerasTensor: shape=(None, 256) dtype=float32 (created by layer 'lstm')>]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "model.layers[2].output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define sampling models\n",
        "# Restore the model and construct the encoder and decoder.\n",
        "encoder_inputs = model.input[0]  # input_1\n",
        "encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output  # lstm_1\n",
        "encoder_states = [state_h_enc, state_c_enc]\n",
        "encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_inputs = model.input[1]  # input_2\n",
        "decoder_state_input_h = keras.Input(shape=(latent_dim,), name=\"input_3\")\n",
        "decoder_state_input_c = keras.Input(shape=(latent_dim,), name=\"input_4\")\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_lstm = model.layers[3]\n",
        "decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h_dec, state_c_dec]\n",
        "decoder_dense = model.layers[4]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = keras.Model(\n",
        "    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nEncoder input (InputLayer)   [(None, None, 96)]        0         \n_________________________________________________________________\nlstm (LSTM)                  [(None, 256), (None, 256) 361472    \n=================================================================\nTotal params: 361,472\nTrainable params: 361,472\nNon-trainable params: 0\n_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "encoder_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\nDecoder input (InputLayer)      [(None, None, 97)]   0                                            \n__________________________________________________________________________________________________\ninput_3 (InputLayer)            [(None, 256)]        0                                            \n__________________________________________________________________________________________________\ninput_4 (InputLayer)            [(None, 256)]        0                                            \n__________________________________________________________________________________________________\nlstm_1 (LSTM)                   [(None, None, 256),  362496      Decoder input[0][0]              \n                                                                 input_3[0][0]                    \n                                                                 input_4[0][0]                    \n__________________________________________________________________________________________________\ndense (Dense)                   (None, None, 97)     24929       lstm_1[1][0]                     \n==================================================================================================\nTotal params: 387,425\nTrainable params: 387,425\nNon-trainable params: 0\n__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "decoder_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "may-aY1w6NUO"
      },
      "source": [
        "# Reverse-lookup token index to decode sequences back to\n",
        "# something readable.\n",
        "#reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "#reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = \"\"\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.0\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "    return decoded_sentence"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLd_D26Dlj78"
      },
      "source": [
        "try_input= \"parut ni surpris ni contrarié de la trovqr partie il\"\n",
        "# try \n",
        "try_correction= np.zeros((1, max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\")\n",
        "for t, char in enumerate(try_input):\n",
        "  try_correction[0, t, input_token_index[char]] = 1.0\n",
        "try_correction[0, t + 1 :, input_token_index[\" \"]] = 1.0"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 79, 96)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "try_correction.shape"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "hj4i5w4DpLzG",
        "outputId": "c74619a1-be19-4d5a-f964-bb45b238f83b"
      },
      "source": [
        "decode_sequence(try_correction)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'parut ni surpris ni contraire de la troppe appelé à\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hidDcIEoq1tN"
      },
      "source": [
        "#pid = os.getpid()\n",
        "#!kill -9 $pid"
      ],
      "execution_count": 40,
      "outputs": []
    }
  ]
}